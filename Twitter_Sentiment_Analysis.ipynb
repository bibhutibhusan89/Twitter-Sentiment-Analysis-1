{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Sentiment Analysis\n",
    "The purpose of this project is to predict sentiment for the given Twitter post using Python."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note: Sentiment analysis can predict many different emotions, but in this project only 3 major were considered: positive, negative and neutral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing the packages\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re as regex\n",
    "import numpy as np\n",
    "import plotly\n",
    "from plotly import graph_objs\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from time import time\n",
    "try:\n",
    "    import gensim\n",
    "except NameError:\n",
    "    print(\"gensim package is not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing the packeges for plotly graph\n",
    "from IPython.core.interactiveshell import InteractiveShell     #An enhanced, interactive shell for Python\n",
    "#‘all’, ‘last’, ‘last_expr’ or ‘none’, ‘last_expr_or_assign’\n",
    "#specifying which nodes should be run interactively \n",
    "import plotly \n",
    "plotly.tools.set_credentials_file(username='KunalBhashkar', api_key='3ImJpD57ThNbPx117FsM')\n",
    "InteractiveShell.ast_node_interactivity = \"all\"   #Options:\t'all','last','last_expr','none','last_expr_or_assign'\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "\n",
    "#Plotly Offline brings interactive Plotly graphs to the offline (local) environment\n",
    "import plotly.offline as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as gobj\n",
    "import plotly.plotly as plty\n",
    "from plotly.graph_objs import *\n",
    "py.init_notebook_mode(connected=True)  # initiate notebook for offline plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "They classify Tweets for a query term into negative or positive sentiment.To collect positive and negative tweets, they query twitter for happy and sad emoticons.\n",
    "\n",
    "Happy emoticons are different versions of smiling face, like \":)\", \":-)\", \": )\", \":D\", \"=)\" etc.\n",
    "Sad emoticons include frowns, like \":(\", \":-(\", \":(\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Emotion class to check whether it is negative or positive\n",
    "class EmoticonDetector:\n",
    "    emoticons = {}\n",
    "\n",
    "    def __init__(self, emoticon_file=\"./coding/ml-twitter-sentiment-analysis-develop/data/emoticons.txt\"):\n",
    "        from pathlib import Path\n",
    "        content = Path(emoticon_file).read_text()\n",
    "        positive = True\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if \"positive\" in line.lower():\n",
    "                positive = True\n",
    "                continue\n",
    "            elif \"negative\" in line.lower():\n",
    "                positive = False\n",
    "                continue\n",
    "\n",
    "            self.emoticons[line] = positive\n",
    "\n",
    "    def is_positive(self, emoticon):\n",
    "        if emoticon in self.emoticons:\n",
    "            return self.emoticons[emoticon]\n",
    "        return False\n",
    "\n",
    "    def is_emoticon(self, to_check):\n",
    "        return to_check in self.emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class to initialize Twitter data\n",
    "class TwitterData_Initialize():\n",
    "    data = []\n",
    "    processed_data = []\n",
    "    wordlist = []\n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    is_testing = False    \n",
    "    def initialize(self, csv_file, is_testing_set=False,from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            self.data_model = pd.read_csv(from_cached)\n",
    "            return\n",
    "\n",
    "        self.is_testing = is_testing_set\n",
    "\n",
    "        if not is_testing_set:\n",
    "            self.data = pd.read_csv(csv_file, header=0, names=[\"id\", \"emotion\", \"text\"])\n",
    "            self.data = self.data[self.data[\"emotion\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "        else:\n",
    "            self.data = pd.read_csv(csv_file, header=0, names=[\"id\", \"text\"],dtype={\"id\":\"int64\",\"text\":\"str\"},nrows=4000)\n",
    "            not_null_text = 1 ^ pd.isnull(self.data[\"text\"])\n",
    "            not_null_id = 1 ^ pd.isnull(self.data[\"id\"])\n",
    "            self.data = self.data.loc[not_null_id & not_null_text, :]\n",
    "\n",
    "        self.processed_data = self.data\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>635769805279248384</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>635930169241374720</td>\n",
       "      <td>neutral</td>\n",
       "      <td>IOS 9 App Transport Security. Mm need to check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>635950258682523648</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Mar if you have an iOS device, you should down...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>636030803433009153</td>\n",
       "      <td>negative</td>\n",
       "      <td>@jimmie_vanagon my phone does not run on lates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>636100906224848896</td>\n",
       "      <td>positive</td>\n",
       "      <td>Not sure how to start your publication on iOS?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id   emotion  \\\n",
       "0  635769805279248384  negative   \n",
       "1  635930169241374720   neutral   \n",
       "2  635950258682523648   neutral   \n",
       "3  636030803433009153  negative   \n",
       "4  636100906224848896  positive   \n",
       "\n",
       "                                                text  \n",
       "0                                      Not Available  \n",
       "1  IOS 9 App Transport Security. Mm need to check...  \n",
       "2  Mar if you have an iOS device, you should down...  \n",
       "3  @jimmie_vanagon my phone does not run on lates...  \n",
       "4  Not sure how to start your publication on iOS?...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading and initialization of twitter data\n",
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"./coding/ml-twitter-sentiment-analysis-develop/data/train.csv\")\n",
    "data.processed_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of Data in negative,positive and neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          "negative",
          "neutral",
          "positive"
         ],
         "y": [
          956,
          2125,
          2888
         ]
        }
       ],
       "layout": {
        "title": "Data Distribution in training set"
       }
      },
      "text/html": [
       "<div id=\"4456cc51-3920-4a63-a90e-329b437e1fce\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"4456cc51-3920-4a63-a90e-329b437e1fce\", [{\"type\": \"bar\", \"x\": [\"negative\", \"neutral\", \"positive\"], \"y\": [956, 2125, 2888]}], {\"title\": \"Data Distribution in training set\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"4456cc51-3920-4a63-a90e-329b437e1fce\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"4456cc51-3920-4a63-a90e-329b437e1fce\", [{\"type\": \"bar\", \"x\": [\"negative\", \"neutral\", \"positive\"], \"y\": [956, 2125, 2888]}], {\"title\": \"Data Distribution in training set\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Distribution of data\n",
    "df = data.processed_data\n",
    "neg = len(df[df[\"emotion\"] == \"negative\"])\n",
    "pos = len(df[df[\"emotion\"] == \"positive\"])\n",
    "neu = len(df[df[\"emotion\"] == \"neutral\"])\n",
    "dist = [\n",
    "    graph_objs.Bar(\n",
    "        x=[\"negative\",\"neutral\",\"positive\"],\n",
    "        y=[neg, neu, pos],\n",
    ")]\n",
    "plotly.offline.iplot({\"data\":dist, \"layout\":graph_objs.Layout(title=\"Data Distribution in training set\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The preprocessing steps are execute as follows:\n",
    "i. Cleansing\n",
    "        Remove URLs\n",
    "        Remove usernames\n",
    "        Remove tweets with Not Available text\n",
    "        Remove special characters\n",
    "        Remove numbers\n",
    "ii. Text processing\n",
    "        Tokenize\n",
    "        Transform to lowercase\n",
    "        Stem\n",
    "iii. Word List building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning process\n",
    "class TwitterCleanuper:\n",
    "    def iterate(self):\n",
    "        for cleanup_method in [self.remove_urls,\n",
    "                               self.remove_usernames,\n",
    "                               self.remove_na,\n",
    "                               self.remove_special_chars,\n",
    "                               self.remove_numbers]:\n",
    "            yield cleanup_method\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_by_regex(tweets, regexp):\n",
    "        tweets.loc[:, \"text\"].replace(regexp, \"\", inplace=True)\n",
    "        return tweets\n",
    "\n",
    "    def remove_urls(self, tweets):\n",
    "        return TwitterCleanuper.remove_by_regex(tweets, regex.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "\n",
    "    def remove_na(self, tweets):\n",
    "        return tweets[tweets[\"text\"] != \"Not Available\"]\n",
    "\n",
    "    def remove_special_chars(self, tweets):  # it unrolls the hashtags to normal words\n",
    "        for remove in map(lambda r: regex.compile(regex.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n",
    "                                                                     \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                     \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                     \"!\", \"?\", \".\", \"'\",\n",
    "                                                                     \"--\", \"---\", \"#\"]):\n",
    "            tweets.loc[:, \"text\"].replace(remove, \"\", inplace=True)\n",
    "        return tweets\n",
    "\n",
    "    def remove_usernames(self, tweets):\n",
    "        return TwitterCleanuper.remove_by_regex(tweets, regex.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "\n",
    "    def remove_numbers(self, tweets):\n",
    "        return TwitterCleanuper.remove_by_regex(tweets, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class to clean the twitter data\n",
    "class TwitterData_Cleansing(TwitterData_Initialize):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "        \n",
    "    def cleanup(self, cleanuper):\n",
    "        t = self.processed_data\n",
    "        for cleanup_method in cleanuper.iterate():\n",
    "            if not self.is_testing:\n",
    "                t = cleanup_method(t)\n",
    "            else:\n",
    "                if cleanup_method.__name__ != \"remove_na\":\n",
    "                    t = cleanup_method(t)\n",
    "\n",
    "        self.processed_data = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunal/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:4619: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>635930169241374720</td>\n",
       "      <td>neutral</td>\n",
       "      <td>IOS App Transport Security Mm need to check if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>635950258682523648</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Mar if you have an iOS device you should downl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>636030803433009153</td>\n",
       "      <td>negative</td>\n",
       "      <td>my phone does not run on latest IOS which may ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>636100906224848896</td>\n",
       "      <td>positive</td>\n",
       "      <td>Not sure how to start your publication on iOS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>636176272947744772</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Two Dollar Tuesday is here with Forklift Quick...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id   emotion  \\\n",
       "1  635930169241374720   neutral   \n",
       "2  635950258682523648   neutral   \n",
       "3  636030803433009153  negative   \n",
       "4  636100906224848896  positive   \n",
       "5  636176272947744772   neutral   \n",
       "\n",
       "                                                text  \n",
       "1  IOS App Transport Security Mm need to check if...  \n",
       "2  Mar if you have an iOS device you should downl...  \n",
       "3  my phone does not run on latest IOS which may ...  \n",
       "4  Not sure how to start your publication on iOS ...  \n",
       "5  Two Dollar Tuesday is here with Forklift Quick...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally cleaning the data\n",
    "data = TwitterData_Cleansing(data)\n",
    "data.cleanup(TwitterCleanuper())\n",
    "data.processed_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization & stemming"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
    "\n",
    "am, are, is $\\Rightarrow$ be \n",
    "car, cars, car's, cars' $\\Rightarrow$ car\n",
    "The result of this mapping of text will be something like:\n",
    "the boy's cars are different colors $\\Rightarrow$ \n",
    "the boy car be differ color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class to Normalize and Tokenize the dataset\n",
    "class TwitterData_TokenStem(TwitterData_Cleansing):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "        \n",
    "    def stem(self, stemmer=nltk.PorterStemmer()):\n",
    "        def stem_and_join(row):\n",
    "            row[\"text\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"text\"]))\n",
    "            return row\n",
    "\n",
    "        self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "\n",
    "    def tokenize(self, tokenizer=nltk.word_tokenize):\n",
    "        def tokenize_row(row):\n",
    "            row[\"text\"] = tokenizer(row[\"text\"])\n",
    "            row[\"tokenized_text\"] = [] + row[\"text\"]\n",
    "            return row\n",
    "\n",
    "        self.processed_data = self.processed_data.apply(tokenize_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>635930169241374720</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[io, app, transport, secur, mm, need, to, chec...</td>\n",
       "      <td>[IOS, App, Transport, Security, Mm, need, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>635950258682523648</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[mar, if, you, have, an, io, devic, you, shoul...</td>\n",
       "      <td>[Mar, if, you, have, an, iOS, device, you, sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>636030803433009153</td>\n",
       "      <td>negative</td>\n",
       "      <td>[my, phone, doe, not, run, on, latest, io, whi...</td>\n",
       "      <td>[my, phone, does, not, run, on, latest, IOS, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>636100906224848896</td>\n",
       "      <td>positive</td>\n",
       "      <td>[not, sure, how, to, start, your, public, on, ...</td>\n",
       "      <td>[Not, sure, how, to, start, your, publication,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>636176272947744772</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[two, dollar, tuesday, is, here, with, forklif...</td>\n",
       "      <td>[Two, Dollar, Tuesday, is, here, with, Forklif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id   emotion  \\\n",
       "1  635930169241374720   neutral   \n",
       "2  635950258682523648   neutral   \n",
       "3  636030803433009153  negative   \n",
       "4  636100906224848896  positive   \n",
       "5  636176272947744772   neutral   \n",
       "\n",
       "                                                text  \\\n",
       "1  [io, app, transport, secur, mm, need, to, chec...   \n",
       "2  [mar, if, you, have, an, io, devic, you, shoul...   \n",
       "3  [my, phone, doe, not, run, on, latest, io, whi...   \n",
       "4  [not, sure, how, to, start, your, public, on, ...   \n",
       "5  [two, dollar, tuesday, is, here, with, forklif...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "1  [IOS, App, Transport, Security, Mm, need, to, ...  \n",
       "2  [Mar, if, you, have, an, iOS, device, you, sho...  \n",
       "3  [my, phone, does, not, run, on, latest, IOS, w...  \n",
       "4  [Not, sure, how, to, start, your, publication,...  \n",
       "5  [Two, Dollar, Tuesday, is, here, with, Forklif...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally process the data for stemming\n",
    "data = TwitterData_TokenStem(data)\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.processed_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3744), ('to', 2477), ('i', 1667), ('a', 1620), ('on', 1557)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method to get most common words from preprocessed data\n",
    "words = Counter()\n",
    "for idx in data.processed_data.index:\n",
    "    words.update(data.processed_data.loc[idx, \"text\"])\n",
    "#Getting most common words \n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('may', 1027), ('tomorrow', 764), ('day', 526), ('go', 499), ('thi', 495)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing the stopwords\n",
    "stopwords=nltk.corpus.stopwords.words(\"english\")\n",
    "whitelist = [\"n't\", \"not\"]\n",
    "for idx, stop_word in enumerate(stopwords):\n",
    "    if stop_word not in whitelist:\n",
    "        del words[stop_word]\n",
    "words.most_common(5)\n",
    "\n",
    "# output\n",
    "# [('may', 1027), ('tomorrow', 764), ('day', 526), ('go', 499), ('thi', 495)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class to build the word list\n",
    "class TwitterData_Wordlist(TwitterData_TokenStem):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "        \n",
    "    whitelist = [\"n't\",\"not\"]\n",
    "    wordlist = []\n",
    "        \n",
    "    def build_wordlist(self, min_occurrences=3, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                       whitelist=None):\n",
    "        self.wordlist = []\n",
    "        whitelist = self.whitelist if whitelist is None else whitelist\n",
    "        import os\n",
    "        if os.path.isfile(\"./coding/ml-twitter-sentiment-analysis-develop/data/wordlist.csv\"):\n",
    "            word_df = pd.read_csv(\"./coding/ml-twitter-sentiment-analysis-develop/data/wordlist.csv\")\n",
    "            word_df = word_df[word_df[\"occurrences\"] > min_occurrences]\n",
    "            self.wordlist = list(word_df.loc[:, \"word\"])\n",
    "            return\n",
    "\n",
    "        words = Counter()\n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"text\"])\n",
    "\n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"./coding/ml-twitter-sentiment-analysis-develop/data/wordlist.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying the method on dataset\n",
    "data = TwitterData_Wordlist(data)\n",
    "data.build_wordlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "orientation": "h",
         "type": "bar",
         "x": [
          285,
          301,
          344,
          348,
          359,
          372,
          374,
          426,
          483,
          495,
          499
         ],
         "y": [
          "one",
          "amp",
          "like",
          "get",
          "time",
          "see",
          "im",
          "not",
          "wa",
          "thi",
          "go"
         ]
        }
       ],
       "layout": {
        "title": "Most common word in wordlist"
       }
      },
      "text/html": [
       "<div id=\"d389a79d-a398-4b38-8292-2d5f342e367c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d389a79d-a398-4b38-8292-2d5f342e367c\", [{\"type\": \"bar\", \"x\": [285, 301, 344, 348, 359, 372, 374, 426, 483, 495, 499], \"y\": [\"one\", \"amp\", \"like\", \"get\", \"time\", \"see\", \"im\", \"not\", \"wa\", \"thi\", \"go\"], \"orientation\": \"h\"}], {\"title\": \"Most common word in wordlist\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d389a79d-a398-4b38-8292-2d5f342e367c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d389a79d-a398-4b38-8292-2d5f342e367c\", [{\"type\": \"bar\", \"x\": [285, 301, 344, 348, 359, 372, 374, 426, 483, 495, 499], \"y\": [\"one\", \"amp\", \"like\", \"get\", \"time\", \"see\", \"im\", \"not\", \"wa\", \"thi\", \"go\"], \"orientation\": \"h\"}], {\"title\": \"Most common word in wordlist\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the words which is most common\n",
    "words = pd.read_csv(\"./coding/ml-twitter-sentiment-analysis-develop/data/wordlist.csv\")\n",
    "x_words = list(words.loc[0:10,\"word\"])\n",
    "x_words.reverse()\n",
    "y_occ = list(words.loc[0:10,\"occurrences\"])\n",
    "y_occ.reverse()\n",
    "\n",
    "dist = [\n",
    "    graph_objs.Bar(\n",
    "        x=y_occ,\n",
    "        y=x_words,\n",
    "        orientation=\"h\"\n",
    ")]\n",
    "py.iplot({\"data\":dist, \"layout\":graph_objs.Layout(title=\"Most common word in wordlist\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words approach"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Most sentiment analysis systems use bag-of-words approach for mining sentiments from the online reviews and social media data. Rather considering the whole sentence/ paragraph for analysis, the bag-of-words approach considers only individual words and their count as the feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class to define twitter data bag of words\n",
    "class TwitterData_BagOfWords(TwitterData_Wordlist):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "        self.wordlist = previous.wordlist\n",
    "    \n",
    "    def build_data_model(self):\n",
    "        label_column = []\n",
    "        if not self.is_testing:\n",
    "            label_column = [\"label\"]\n",
    "\n",
    "        columns = label_column + list(\n",
    "            map(lambda w: w + \"_bow\",self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            if not self.is_testing:\n",
    "                # add label\n",
    "                current_label = self.processed_data.loc[idx, \"emotion\"]\n",
    "                labels.append(current_label)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>go_bow</th>\n",
       "      <th>thi_bow</th>\n",
       "      <th>wa_bow</th>\n",
       "      <th>not_bow</th>\n",
       "      <th>im_bow</th>\n",
       "      <th>see_bow</th>\n",
       "      <th>time_bow</th>\n",
       "      <th>get_bow</th>\n",
       "      <th>like_bow</th>\n",
       "      <th>...</th>\n",
       "      <th>leadership_bow</th>\n",
       "      <th>snp_bow</th>\n",
       "      <th>tsiprass_bow</th>\n",
       "      <th>parliamentari_bow</th>\n",
       "      <th>alexi_bow</th>\n",
       "      <th>farag_bow</th>\n",
       "      <th>girlfriend_bow</th>\n",
       "      <th>castl_bow</th>\n",
       "      <th>crasher_bow</th>\n",
       "      <th>fiddl_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  go_bow  thi_bow  wa_bow  not_bow  im_bow  see_bow  time_bow  \\\n",
       "0   neutral       0        0       0        0       0        0         0   \n",
       "1   neutral       0        0       0        0       0        0         0   \n",
       "2  negative       0        0       1        1       0        0         1   \n",
       "3  positive       0        0       0        1       0        0         0   \n",
       "4   neutral       0        0       0        0       0        0         0   \n",
       "\n",
       "   get_bow  like_bow    ...      leadership_bow  snp_bow  tsiprass_bow  \\\n",
       "0        0         0    ...                   0        0             0   \n",
       "1        0         0    ...                   0        0             0   \n",
       "2        0         0    ...                   0        0             0   \n",
       "3        0         0    ...                   0        0             0   \n",
       "4        0         0    ...                   0        0             0   \n",
       "\n",
       "   parliamentari_bow  alexi_bow  farag_bow  girlfriend_bow  castl_bow  \\\n",
       "0                  0          0          0               0          0   \n",
       "1                  0          0          0               0          0   \n",
       "2                  0          0          0               0          0   \n",
       "3                  0          0          0               0          0   \n",
       "4                  0          0          0               0          0   \n",
       "\n",
       "   crasher_bow  fiddl_bow  \n",
       "0            0          0  \n",
       "1            0          0  \n",
       "2            0          0  \n",
       "3            0          0  \n",
       "4            0          0  \n",
       "\n",
       "[5 rows x 2185 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#processing bag of words\n",
    "data = TwitterData_BagOfWords(data)\n",
    "bow, labels = data.build_data_model()\n",
    "bow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "positive",
         "type": "bar",
         "x": [
          "thi",
          "go",
          "see",
          "im",
          "wa",
          "time",
          "friday",
          "not",
          "like",
          "trump",
          "plan",
          "obama",
          "get"
         ],
         "y": [
          254,
          247,
          244,
          210,
          187,
          182,
          169,
          136,
          127,
          28,
          30,
          55,
          161
         ]
        },
        {
         "name": "negative",
         "type": "bar",
         "x": [
          "thi",
          "go",
          "see",
          "im",
          "wa",
          "time",
          "friday",
          "not",
          "like",
          "trump",
          "plan",
          "obama",
          "get"
         ],
         "y": [
          80,
          51,
          37,
          46,
          89,
          46,
          17,
          103,
          72,
          61,
          60,
          57,
          54
         ]
        },
        {
         "name": "neutral",
         "type": "bar",
         "x": [
          "thi",
          "go",
          "see",
          "im",
          "wa",
          "time",
          "friday",
          "not",
          "like",
          "trump",
          "plan",
          "obama",
          "get"
         ],
         "y": [
          145,
          153,
          78,
          89,
          156,
          113,
          81,
          169,
          131,
          74,
          48,
          77,
          118
         ]
        }
       ],
       "layout": {
        "title": "Most common words across sentiments"
       }
      },
      "text/html": [
       "<div id=\"6e8932ae-90df-4988-99a3-2cb5e04675c7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6e8932ae-90df-4988-99a3-2cb5e04675c7\", [{\"type\": \"bar\", \"x\": [\"thi\", \"go\", \"see\", \"im\", \"wa\", \"time\", \"friday\", \"not\", \"like\", \"trump\", \"plan\", \"obama\", \"get\"], \"y\": [254, 247, 244, 210, 187, 182, 169, 136, 127, 28, 30, 55, 161], \"name\": \"positive\"}, {\"type\": \"bar\", \"x\": [\"thi\", \"go\", \"see\", \"im\", \"wa\", \"time\", \"friday\", \"not\", \"like\", \"trump\", \"plan\", \"obama\", \"get\"], \"y\": [80, 51, 37, 46, 89, 46, 17, 103, 72, 61, 60, 57, 54], \"name\": \"negative\"}, {\"type\": \"bar\", \"x\": [\"thi\", \"go\", \"see\", \"im\", \"wa\", \"time\", \"friday\", \"not\", \"like\", \"trump\", \"plan\", \"obama\", \"get\"], \"y\": [145, 153, 78, 89, 156, 113, 81, 169, 131, 74, 48, 77, 118], \"name\": \"neutral\"}], {\"title\": \"Most common words across sentiments\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6e8932ae-90df-4988-99a3-2cb5e04675c7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6e8932ae-90df-4988-99a3-2cb5e04675c7\", [{\"type\": \"bar\", \"x\": [\"thi\", \"go\", \"see\", \"im\", \"wa\", \"time\", \"friday\", \"not\", \"like\", \"trump\", \"plan\", \"obama\", \"get\"], \"y\": [254, 247, 244, 210, 187, 182, 169, 136, 127, 28, 30, 55, 161], \"name\": \"positive\"}, {\"type\": \"bar\", \"x\": [\"thi\", \"go\", \"see\", \"im\", \"wa\", \"time\", \"friday\", \"not\", \"like\", \"trump\", \"plan\", \"obama\", \"get\"], \"y\": [80, 51, 37, 46, 89, 46, 17, 103, 72, 61, 60, 57, 54], \"name\": \"negative\"}, {\"type\": \"bar\", \"x\": [\"thi\", \"go\", \"see\", \"im\", \"wa\", \"time\", \"friday\", \"not\", \"like\", \"trump\", \"plan\", \"obama\", \"get\"], \"y\": [145, 153, 78, 89, 156, 113, 81, 169, 131, 74, 48, 77, 118], \"name\": \"neutral\"}], {\"title\": \"Most common words across sentiments\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Grouped the bag of words according to label\n",
    "grouped = bow.groupby([\"label\"]).sum()\n",
    "words_to_visualize = []\n",
    "sentiments = [\"positive\",\"negative\",\"neutral\"]\n",
    "#get the most 7 common words for every sentiment\n",
    "for sentiment in sentiments:\n",
    "    words = grouped.loc[sentiment,:]\n",
    "    words.sort_values(inplace=True,ascending=False)\n",
    "    for w in words.index[:7]:\n",
    "        if w not in words_to_visualize:\n",
    "            words_to_visualize.append(w)\n",
    "            \n",
    "            \n",
    "#visualize it\n",
    "plot_data = []\n",
    "for sentiment in sentiments:\n",
    "    plot_data.append(graph_objs.Bar(\n",
    "            x = [w.split(\"_\")[0] for w in words_to_visualize],\n",
    "            y = [grouped.loc[sentiment,w] for w in words_to_visualize],\n",
    "            name = sentiment\n",
    "    ))\n",
    "    \n",
    "py.iplot({\n",
    "        \"data\":plot_data,\n",
    "        \"layout\":graph_objs.Layout(title=\"Most common words across sentiments\")\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class to detect emotion\n",
    "class EmoticonDetector:\n",
    "    emoticons = {}\n",
    "\n",
    "    def __init__(self, emoticon_file=\"./coding/ml-twitter-sentiment-analysis-develop/data/emoticons.txt\"):\n",
    "        from pathlib import Path\n",
    "        content = Path(emoticon_file).read_text()\n",
    "        positive = True\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if \"positive\" in line.lower():\n",
    "                positive = True\n",
    "                continue\n",
    "            elif \"negative\" in line.lower():\n",
    "                positive = False\n",
    "                continue\n",
    "\n",
    "            self.emoticons[line] = positive\n",
    "\n",
    "    def is_positive(self, emoticon):\n",
    "        if emoticon in self.emoticons:\n",
    "            return self.emoticons[emoticon]\n",
    "        return False\n",
    "\n",
    "    def is_emoticon(self, to_check):\n",
    "        return to_check in self.emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a class for feature extraction purpose\n",
    "class TwitterData_ExtraFeatures(TwitterData_Wordlist):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_data_model(self):\n",
    "        extra_columns = [col for col in self.processed_data.columns if col.startswith(\"number_of\")]\n",
    "        label_column = []\n",
    "        if not self.is_testing:\n",
    "            label_column = [\"label\"]\n",
    "\n",
    "        columns = label_column + extra_columns + list(\n",
    "            map(lambda w: w + \"_bow\",self.wordlist))\n",
    "        \n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            if not self.is_testing:\n",
    "                # add label\n",
    "                current_label = self.processed_data.loc[idx, \"emotion\"]\n",
    "                labels.append(current_label)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            for _, col in enumerate(extra_columns):\n",
    "                current_row.append(self.processed_data.loc[idx, col])\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels\n",
    "    \n",
    "    def build_features(self):\n",
    "        def count_by_lambda(expression, word_array):\n",
    "            return len(list(filter(expression, word_array)))\n",
    "\n",
    "        def count_occurences(character, word_array):\n",
    "            counter = 0\n",
    "            for j, word in enumerate(word_array):\n",
    "                for char in word:\n",
    "                    if char == character:\n",
    "                        counter += 1\n",
    "\n",
    "            return counter\n",
    "\n",
    "        def count_by_regex(regex, plain_text):\n",
    "            return len(regex.findall(plain_text))\n",
    "\n",
    "        self.add_column(\"splitted_text\", map(lambda txt: txt.split(\" \"), self.processed_data[\"text\"]))\n",
    "\n",
    "        # number of uppercase words\n",
    "        uppercase = list(map(lambda txt: count_by_lambda(lambda word: word == word.upper(), txt),\n",
    "                             self.processed_data[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_uppercase\", uppercase)\n",
    "\n",
    "        # number of !\n",
    "        exclamations = list(map(lambda txt: count_occurences(\"!\", txt),\n",
    "                                self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_exclamation\", exclamations)\n",
    "\n",
    "        # number of ?\n",
    "        questions = list(map(lambda txt: count_occurences(\"?\", txt),\n",
    "                             self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_question\", questions)\n",
    "\n",
    "        # number of ...\n",
    "        ellipsis = list(map(lambda txt: count_by_regex(regex.compile(r\"\\.\\s?\\.\\s?\\.\"), txt),\n",
    "                            self.processed_data[\"text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_ellipsis\", ellipsis)\n",
    "\n",
    "        # number of hashtags\n",
    "        hashtags = list(map(lambda txt: count_occurences(\"#\", txt),\n",
    "                            self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_hashtags\", hashtags)\n",
    "\n",
    "        # number of mentions\n",
    "        mentions = list(map(lambda txt: count_occurences(\"@\", txt),\n",
    "                            self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_mentions\", mentions)\n",
    "\n",
    "        # number of quotes\n",
    "        quotes = list(map(lambda plain_text: int(count_occurences(\"'\", [plain_text.strip(\"'\").strip('\"')]) / 2 +\n",
    "                                                 count_occurences('\"', [plain_text.strip(\"'\").strip('\"')]) / 2),\n",
    "                          self.processed_data[\"text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_quotes\", quotes)\n",
    "\n",
    "        # number of urls\n",
    "        urls = list(map(lambda txt: count_by_regex(regex.compile(r\"http.?://[^\\s]+[\\s]?\"), txt),\n",
    "                        self.processed_data[\"text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_urls\", urls)\n",
    "\n",
    "        # number of positive emoticons\n",
    "        ed = EmoticonDetector()\n",
    "        positive_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and ed.is_positive(word), txt),\n",
    "                self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_positive_emo\", positive_emo)\n",
    "\n",
    "        # number of negative emoticons\n",
    "        negative_emo = list(map(\n",
    "            lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and not ed.is_positive(word), txt),\n",
    "            self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_negative_emo\", negative_emo)\n",
    "        \n",
    "    def add_column(self, column_name, column_content):\n",
    "        self.processed_data.loc[:, column_name] = pd.Series(column_content, index=self.processed_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunal/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:4619: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Twitter data Feature Extraction\n",
    "data = TwitterData_ExtraFeatures()\n",
    "data.initialize(\"./coding/ml-twitter-sentiment-analysis-develop/data/train.csv\")\n",
    "data.build_features()\n",
    "data.cleanup(TwitterCleanuper())\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.build_wordlist()\n",
    "data_model, labels = data.build_data_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification of Twitter dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing the packages\n",
    "import random\n",
    "seed = 666\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method for test classification\n",
    "def test_classifier(X_train, y_train, X_test, y_test, classifier):\n",
    "    print(\"\")\n",
    "    print(\"******Classification of Twitter Sentiment Analysis******\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    print(\"Testing \" + classifier_name)\n",
    "    now = time()\n",
    "    list_of_labels = sorted(list(set(y_train)))\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    print(\"Learing time {0}s\".format(time() - now))\n",
    "    now = time()\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"Predicting time {0}s\".format(time() - now))\n",
    "\n",
    "    precision = precision_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    recall = recall_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    print(\"************ Results ************\")\n",
    "    print(\"            Negative     Neutral     Positive\")\n",
    "    print(\"F1       \" + str(f1))\n",
    "    print(\"Precision\" + str(precision))\n",
    "    print(\"Recall   \" + str(recall))\n",
    "    print(\"Accuracy \" + str(accuracy)) \n",
    "\n",
    "    return precision, recall, accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier as XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunal/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******Classification of Twitter Sentiment Analysis******\n",
      "Testing XGBClassifier\n",
      "Learing time 70.33846664428711s\n",
      "Predicting time 0.1466081142425537s\n",
      "************ Results ************\n",
      "            Negative     Neutral     Positive\n",
      "F1       [0.16716418 0.41090555 0.69450317]\n",
      "Precision[0.37837838 0.47845805 0.59082734]\n",
      "Recall   [0.10727969 0.36006826 0.84230769]\n",
      "Accuracy 0.5507068223724647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunal/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_model.iloc[:, 1:], data_model.iloc[:, 0],\n",
    "                                                    train_size=0.7, stratify=data_model.iloc[:, 0],\n",
    "                                                    random_state=seed)\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, XGBoostClassifier(seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method to find cross validating accuracy as below\n",
    "def cv(classifier, X_train, y_train): \n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    now = time()\n",
    "    print(\"Crossvalidating \" + classifier_name + \"...\")\n",
    "    accuracy = [cross_val_score(classifier, X_train, y_train, cv=8, n_jobs=-1)]\n",
    "    print(\"Crosvalidation completed in {0}s\".format(time() - now))\n",
    "    print(\"Accuracy: \" + str(accuracy[0]))\n",
    "    print(\"Average accuracy: \" + str(np.array(accuracy[0]).mean()))    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossvalidating XGBClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunal/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n",
      "/home/kunal/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n",
      "/home/kunal/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n",
      "/home/kunal/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n",
      "/home/kunal/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n",
      "/home/kunal/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n",
      "/home/kunal/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n",
      "/home/kunal/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crosvalidation completed in 361.70437574386597s\n",
      "Accuracy: [0.53608247 0.47787611 0.39823009 0.30383481 0.34070796 0.50812408\n",
      " 0.47119645 0.53994083]\n",
      "Average accuracy: 0.44699910024231854\n"
     ]
    }
   ],
   "source": [
    "xgb_acc = cv(XGBoostClassifier(seed=seed),data_model.iloc[:, 1:], data_model.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8a036101d0>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Recall')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Precision')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Precision Recall Curve')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8a0345bc88>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAbkE7uj84AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8a032c6860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(recall, precision, label='area = %0.8f' % accuracy, color=\"green\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
